{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63444ecb",
   "metadata": {},
   "source": [
    "<H2><center>Transaction Risk Scoring and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c3bc24",
   "metadata": {},
   "source": [
    "<H4>Description:</H4> This notebook utilizes multi-shot in-context learning with synthetic transaction data to train Mixtral-8x7B-Instruct-v0.1 for risk scoring financial transactions. The model conducts analysis by assessing transaction risk based on extracted entities. It incorporates transaction details along with data from external sources such as the SEC EDGAR database, the OFAC Sanctions List, WikiData, and recent news articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5fe324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "from thefuzz import fuzz, process\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityExtractor:\n",
    "    def __init__(self, base_url=\"https://openrouter.ai/api/v1\"):\n",
    "        self.client = OpenAI(\n",
    "            base_url= base_url,\n",
    "            api_key= os.getenv(\"OpenRouter_API_key\"))\n",
    "        self.model = \"mistralai/mistral-small-3.1-24b-instruct:free\"\n",
    "        self.max_retries = 5\n",
    "        \n",
    "    def extract_entities(self, transaction_text: str) -> list:\n",
    "        prompt_text = f\"\"\"\n",
    "            Extract all named entities from the following transaction text and classify each entity into one of these categories:\n",
    "            Person, Politically Exposed Person, Corporation, Bank, Government Agency, Non-Profit Organization, or Shell Company.\n",
    "            - If the entity's category is ambiguous, classify it as \"Corporation\".\n",
    "            - Do NOT include IBANs, VPNs, IPs, addresses, account numbers, tax IDs, location, cities, or countries.\n",
    "            - Banks should be extracted separately (e.g., \"Swiss Bank\", \"Cayman National Bank\") without IBAN/account numbers.\n",
    "            - Include people with titles (Mr., Mrs., Dr., etc.) as Person.\n",
    "            - Output only a JSON list of objects. Each object must have exactly two keys: \n",
    "              \"entity\" (the entity name) and \"category\" (the classified category).\n",
    "            - Do not output into code block, print as raw text\n",
    "\n",
    "            Transaction Text:\n",
    "            {transaction_text}\n",
    "\n",
    "            Output Format **(Do not output into code block, print as raw text):**\n",
    "            Eg: \n",
    "            [\n",
    "                {{\n",
    "                    \"entity\": \"Acme Corp\",\n",
    "                    \"category\": \"Corporation\"\n",
    "                }},\n",
    "                {{\n",
    "                    \"entity\": \"SovCo Capital Partners\",\n",
    "                    \"category\": \"Corporation\"\n",
    "                }}\n",
    "            ]\n",
    "        \"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                completion = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "                        }\n",
    "                    ]\n",
    "                )\n",
    "                extracted_text = completion.choices[0].message.content\n",
    "                extracted_entities = json.loads(extracted_text)\n",
    "                return extracted_entities\n",
    "\n",
    "            except Exception:\n",
    "                lines = extracted_text.splitlines()\n",
    "                if len(lines) > 2:\n",
    "                    trimmed = \"\\n\".join(lines[1:-1]).strip()\n",
    "                    try:\n",
    "                        extracted_entities = json.loads(trimmed)\n",
    "                        return extracted_entities\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "\n",
    "        return {\"error\": \"Failed to parse entity extraction response\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "268b092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanyMatcher:\n",
    "    def __init__(self, database):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.raw_database = database  \n",
    "        self.database = [self.preprocess_text(name) for name in database]  # Preprocessed database\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        if self.database:\n",
    "            self.vectors = self.vectorizer.fit_transform(self.database)\n",
    "        else:\n",
    "            self.vectors = None \n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z0-9\\s]', '', text)  \n",
    "        text = re.sub(r'\\s+', ' ', text).strip() \n",
    "        \n",
    "        stopwords = {\"corporation\", \"limited\", \"ltd\", \"solutions\", \"technologies\", \n",
    "                     \"consulting\", \"consultancy\", \"services\", \"systems\", \"group\", \n",
    "                     \"inc\", \"pvt\", \"plc\", \"co\"}\n",
    "        \n",
    "        words = text.split()\n",
    "        words = [self.lemmatizer.lemmatize(word) for word in words if word not in stopwords]  # Apply lemmatization\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def get_top_cosine_matches(self, query, top_n=5):\n",
    "        if not self.database:  \n",
    "            return []\n",
    "\n",
    "        query_cleaned = self.preprocess_text(query)\n",
    "        query_vector = self.vectorizer.transform([query_cleaned])\n",
    "        similarity_scores = cosine_similarity(query_vector, self.vectors).flatten()\n",
    "\n",
    "        top_n = min(top_n, len(similarity_scores)) \n",
    "        top_indices = np.argsort(-similarity_scores)[:top_n]\n",
    "        top_matches = [(self.raw_database[i], similarity_scores[i]) for i in top_indices]\n",
    "\n",
    "        return top_matches\n",
    "\n",
    "    def apply_fuzzy_matching(self, query, candidates):\n",
    "        if not candidates:\n",
    "            return None, None\n",
    "\n",
    "        query_cleaned = self.preprocess_text(query)\n",
    "        best_match, best_score = max(\n",
    "            ((name, fuzz.ratio(query_cleaned, self.preprocess_text(name))) for name, _ in candidates),\n",
    "            key=lambda x: x[1]\n",
    "        )\n",
    "\n",
    "        return (best_match, best_score)\n",
    "\n",
    "    def find_best_match(self, query, top_n=5):\n",
    "        top_matches = self.get_top_cosine_matches(query, top_n)\n",
    "        best_match, best_score = self.apply_fuzzy_matching(query, top_matches)\n",
    "        \n",
    "        return best_match, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aebce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanyScreening:\n",
    "    def __init__(self, ofac_list_file):\n",
    "        self.SEC_BASE_URL = \"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "        self.WIKIDATA_URL = \"https://query.wikidata.org/sparql\"\n",
    "        self.NEWS_API_KEY = os.getenv(\"News_API_key\")\n",
    "        self.OFAC_LIST_FILE = ofac_list_file\n",
    "        with open(self.OFAC_LIST_FILE, \"r\") as f:\n",
    "            self.ofac_companies = [line.strip().lower() for line in f]\n",
    "        self.matcher = CompanyMatcher(self.ofac_companies)\n",
    "        \n",
    "    def check_sec_edgar(self, company_name):\n",
    "        params = {\"action\": \"getcompany\", \"company\": company_name, \"output\": \"atom\"}\n",
    "        headers = {\"User-Agent\": \"XXX (xxx@yyy.com)\"}\n",
    "        response = requests.get(self.SEC_BASE_URL, params = params, headers = headers)\n",
    "        if response.status_code == 200 and \"No matching companies\" not in response.text:\n",
    "            soup = BeautifulSoup(response.text, \"xml\")\n",
    "            cik_tag = soup.find(\"cik\")\n",
    "            edgar_profile_url = None\n",
    "            recent_8k_filings = []\n",
    "            \n",
    "            if cik_tag:\n",
    "                cik = cik_tag.text.strip()\n",
    "\n",
    "            three_years_ago = datetime.now() - timedelta(days=3*365)\n",
    "            filings = soup.find_all(\"entry\")\n",
    "\n",
    "            for filing in filings:\n",
    "                title = filing.find(\"title\").text\n",
    "                date_str = filing.find(\"updated\").text[:10]\n",
    "                filing_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "                if \"8-K\" in title and filing_date >= three_years_ago:\n",
    "                    recent_8k_filings.append(title)\n",
    "\n",
    "            return {\n",
    "                \"SEC Registered\": \"Yes\",\n",
    "                \"CIK\": cik if cik_tag else \"Not available\",\n",
    "                \"Recent 8-K Filings\": recent_8k_filings[:3] if recent_8k_filings else \"None\"\n",
    "            }\n",
    "        return {\n",
    "            \"SEC Registered\": \"No\"\n",
    "        }\n",
    "\n",
    "    def check_ofac_sanctions(self, company_name, threshold=85):\n",
    "        match, score = self.matcher.find_best_match(company_name)\n",
    "        return {\n",
    "            \"OFAC Sanctioned\": \"Yes\" if score >= threshold else \"No\",\n",
    "            \"Closest OFAC Database Match\": match if score >= threshold else \"None\",\n",
    "        }\n",
    "\n",
    "    def check_wikidata_scandals(self, company_name):\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT ?company ?companyLabel ?industry ?industryLabel ?scandal ?scandalLabel ?description WHERE {{\n",
    "          ?company rdfs:label \"{company_name}\"@en.\n",
    "          ?company wdt:P31 wd:Q4830453.  # Instance of (Business/Company)\n",
    "          OPTIONAL {{ ?company wdt:P452 ?industry. }}  # Industry type\n",
    "          OPTIONAL {{ ?company schema:description ?description. FILTER (LANG(?description) = \"en\") }}\n",
    "\n",
    "          # Looking for scandals\n",
    "          OPTIONAL {{ ?company wdt:P793 ?scandal. }}  # Significant events (may include fraud cases, controversies)\n",
    "          OPTIONAL {{ ?company wdt:P5053 ?scandal. }} # Cause of dissolution (bankruptcy, fraud)\n",
    "          OPTIONAL {{ ?company wdt:P2416 ?scandal. }} # Scandals (direct connection)\n",
    "\n",
    "          SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        response = requests.get(self.WIKIDATA_URL, params={\"query\": query, \"format\": \"json\"})\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            return {\"Error\": \"Failed to fetch data\"}\n",
    "\n",
    "        data = response.json().get(\"results\", {}).get(\"bindings\", [])\n",
    "\n",
    "        if not data:\n",
    "            return {\"Status\": \"Not Found in Wikidata\"}\n",
    "\n",
    "        result = data[0] \n",
    "        company_qid = result[\"company\"][\"value\"].split(\"/\")[-1]\n",
    "        \n",
    "        description = result.get(\"description\", {}).get(\"value\", \"No description available\")\n",
    "        if \"scandal\" in result:\n",
    "            scandal_name = result[\"scandalLabel\"][\"value\"]\n",
    "            scandal_qid = result[\"scandal\"][\"value\"].split(\"/\")[-1]\n",
    "            scandal_link = f\"https://www.wikidata.org/wiki/{scandal_qid}\"\n",
    "\n",
    "        return {\n",
    "            \"Wikidata_QID\": company_qid,\n",
    "            \"Description\": description,\n",
    "            \"Scandals\": (scandal_name, scandal_link) if \"scandal\" in result else \"No known scandals\",\n",
    "        }\n",
    "    def get_news(self, company_name):\n",
    "        url = f\"https://newsapi.org/v2/everything?q={company_name}&language=en&sortBy=publishedAt&apiKey={self.NEWS_API_KEY}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            return {\"Recent News\": \"Error fetching news\"}\n",
    "\n",
    "        news_data = response.json()\n",
    "        articles = news_data.get(\"articles\", [])\n",
    "\n",
    "        filtered_articles = [article[\"title\"] for article in articles if company_name.lower() in article[\"title\"].lower()]\n",
    "        return {\"Recent News\": filtered_articles[:3] if filtered_articles else \"No relevant news found\"}\n",
    "\n",
    "    def screen_company(self, company_name):\n",
    "        result = {\"Company\": company_name}\n",
    "        result.update(self.check_sec_edgar(company_name))\n",
    "        result.update(self.check_ofac_sanctions(company_name))\n",
    "        result.update(self.check_wikidata_scandals(company_name))\n",
    "        result.update(self.get_news(company_name))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f50794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransactionAnalyzer:\n",
    "    def __init__(self, ofac_list_file):\n",
    "        self.entity_extractor = EntityExtractor()\n",
    "        self.company_screening = CompanyScreening(ofac_list_file)\n",
    "\n",
    "    def analyze_transaction(self, transaction_text):\n",
    "        extracted_entities = self.entity_extractor.extract_entities(transaction_text)\n",
    "        final_results = {\"Corporation Details\": []}\n",
    "\n",
    "        for entity in extracted_entities:\n",
    "            if entity[\"category\"] == \"Corporation\":\n",
    "                company_details = self.company_screening.screen_company(entity[\"entity\"])\n",
    "                final_results[\"Corporation Details\"].append(company_details)\n",
    "\n",
    "        return json.dumps(final_results, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.api_key = os.getenv(\"TogetherAI_API_Key\") \n",
    "        self.api_url = \"https://api.together.xyz/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "    def analyze_risk(self, input_texts, output_texts, test_input):\n",
    "\n",
    "        examples = random.sample(list(zip(input_texts, output_texts)), 50)\n",
    "        icl_prompt = \"\".join([f\"Input: {inp}\\nOutput: {out}\\n\\n\" for inp, out in examples])\n",
    "        icl_prompt += f\"Input: {test_input}\\n Output:\"\n",
    "\n",
    "        data = {\n",
    "            \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": icl_prompt}],\n",
    "        }\n",
    "        \n",
    "        response = requests.post(self.api_url, headers=self.headers, json=data)\n",
    "        \n",
    "        try:\n",
    "            return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except KeyError:\n",
    "            return f\"Error: {response.json()}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b7bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corporate_details.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "    details= [t.strip() for t in content.split('---') if t.strip()]\n",
    "    \n",
    "with open(\"synthetic_unstructured_data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "    transactions = [t.strip() for t in content.split('---') if t.strip()]\n",
    "    \n",
    "with open(\"risk_summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "    risk= [t.strip() for t in content.split('---') if t.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "239e8758",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_inputs = [f\"{a1} {a2}\" for a1, a2 in zip(transactions[:56], details[:56])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "155c8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unstructured_data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "    transactions = [t.strip() for t in content.split('---') if t.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "612d0b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"structured_data.csv\", newline = \"\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if \"TransactionText\" in row:\n",
    "            txn_text = row[\"TransactionText\"]\n",
    "        else:\n",
    "            txn_text = \"\\n\".join(f\"{key}: {value}\" for key, value in row.items())\n",
    "            transactions.append(txn_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2c3017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = TransactionAnalyzer(\"ofac_list.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "923d0ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Risk Score: 0.75\n",
      "\n",
      "Reason: The transaction involves a consulting fee paid to a company in Switzerland, with the funds processed via an intermediary in the British Virgin Islands (BVI), a jurisdiction known for its weak financial regulations and transparency. The sender's IP address shows a VPN connection from Panama, adding to the concerns about potential money laundering or tax evasion. Although Global Horizona Consulting LLC is not sanctioned, the unusual circumstances surrounding the transaction warrant a higher risk score.\n"
     ]
    }
   ],
   "source": [
    "result_json = analyzer.analyze_transaction(transactions[0])\n",
    "risk_model_input = f\"{transactions[0]}\\n{result_json}\"\n",
    "analyzer = RiskAnalyzer()\n",
    "result = analyzer.analyze_risk(combined_inputs, risk, risk_model_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fac7be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Risk Score: 0.9\n",
      "Reason: The transaction involves Quantum Holdings Ltd, a company with a beneficiary owner linked to the OFAC SDN List in 2022. The funds are routed through Deutsche Bank Frankfurt and Emirates NBD Dubai, raising concerns about potential money laundering. The approval by Mr. Viktor Petzov, who is linked to the OFAC SDN List, further increases the risk level.\n"
     ]
    }
   ],
   "source": [
    "analyzer = TransactionAnalyzer(\"ofac_list.txt\")\n",
    "result_json = analyzer.analyze_transaction(transactions[1])\n",
    "risk_model_input = f\"{transactions[1]}\\n{result_json}\"\n",
    "result = RiskAnalyzer().analyze_risk(combined_inputs[:56], risk, risk_model_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a6c0a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Risk Score: 0.2\n",
      "Reason: Legitimate business transaction between two registered corporations in the same country, no red flags detected.\n"
     ]
    }
   ],
   "source": [
    "result_json = analyzer.analyze_transaction(transactions[2])\n",
    "risk_model_input = f\"{transactions[2]}\\n{result_json}\"\n",
    "analyzer = RiskAnalyzer()\n",
    "result = analyzer.analyze_risk(combined_inputs[:56], risk, risk_model_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8070616f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Risk Score: 0.2\n",
      "Reason: Legitimate grant disbursement from a reputable global health foundation to a well-known charity organization. No red flags detected.\n"
     ]
    }
   ],
   "source": [
    "analyzer = TransactionAnalyzer(\"ofac_list.txt\")\n",
    "result_json = analyzer.analyze_transaction(transactions[3])\n",
    "risk_model_input = f\"{transactions[3]}\\n{result_json}\"\n",
    "analyzer = RiskAnalyzer()\n",
    "result = analyzer.analyze_risk(combined_inputs[:56], risk, risk_model_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "224ae28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Risk Score: 0.2\n",
      "Reason: Standard business transaction between two registered entities for the purchase of office supplies.\n"
     ]
    }
   ],
   "source": [
    "analyzer = TransactionAnalyzer(\"ofac_list.txt\")\n",
    "result_json = analyzer.analyze_transaction(transactions[4])\n",
    "risk_model_input = f\"{transactions[4]}\\n{result_json}\"\n",
    "analyzer = RiskAnalyzer()\n",
    "result = analyzer.analyze_risk(combined_inputs[:56], risk, risk_model_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "030f43de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Risk Score: 0.5\n",
      "Reason: Large donation from a US-based organization to a Cayman Islands-based entity. Further scrutiny required due to the Cayman Islands' reputation as a tax haven.\n"
     ]
    }
   ],
   "source": [
    "analyzer = TransactionAnalyzer(\"ofac_list.txt\")\n",
    "result_json = analyzer.analyze_transaction(transactions[5])\n",
    "risk_model_input = f\"{transactions[5]}\\n{result_json}\"\n",
    "analyzer = RiskAnalyzer()\n",
    "result = analyzer.analyze_risk(combined_inputs[:56], risk, risk_model_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e1e6e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Risk Score: 0.9\n",
      "Reason: Oceanic Holdings LLC is OFAC sanctioned, raising significant concerns about this offshore investment in Panama.\n"
     ]
    }
   ],
   "source": [
    "analyzer = TransactionAnalyzer(\"ofac_list.txt\")\n",
    "result_json = analyzer.analyze_transaction(transactions[6])\n",
    "risk_model_input = f\"{transactions[6]}\\n{result_json}\"\n",
    "analyzer = RiskAnalyzer()\n",
    "result = analyzer.analyze_risk(combined_inputs[:56], risk, risk_model_input)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
