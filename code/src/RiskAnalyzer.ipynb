{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63444ecb",
   "metadata": {},
   "source": [
    "<H2><center>Transaction Risk Scoring and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c3bc24",
   "metadata": {},
   "source": [
    "<H4>Description:</H4> This notebook utilizes multi-shot in-context learning with synthetic transaction data to train Mixtral-8x7B-Instruct-v0.1 for risk scoring financial transactions. The model conducts analysis by assessing transaction risk based on extracted entities. It incorporates transaction details along with data from external sources such as the SEC EDGAR database, the OFAC Sanctions List, WikiData, and recent news articles. The provided model confidence scores are only based on the availability of external data sources (SEC EDGAR, WikiData etc). For detailed calculation of actual confidence score, refer Uncertainty_Quantification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5fe324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "from thefuzz import fuzz, process\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityExtractor:\n",
    "    def __init__(self, base_url=\"https://openrouter.ai/api/v1\"):\n",
    "        self.client = OpenAI(\n",
    "            base_url=base_url,\n",
    "            api_key=os.getenv(OpenRouter_API_Key))\n",
    "        self.model = \"mistralai/mistral-small-3.1-24b-instruct:free\"\n",
    "        self.max_retries = 5\n",
    "        \n",
    "    def extract_entities(self, transaction_text: str) -> list:\n",
    "        prompt_text = f\"\"\"\n",
    "            Extract transaction id and all named entities from the following transaction text and classify each entity into one of these categories:\n",
    "            Person, Politically Exposed Person, Corporation, Bank, Government Agency, Non-Profit Organization, or Shell Company.\n",
    "            - If the entity's category is ambiguous, classify it as \"Corporation\".\n",
    "            - Do NOT include IBANs, VPNs, IPs, addresses, account numbers, tax IDs, location, cities, or countries.\n",
    "            - Banks should be extracted separately (e.g., \"Swiss Bank\", \"Cayman National Bank\") without IBAN/account numbers.\n",
    "            - Include people with titles (Mr., Mrs., Dr., etc.) as Person.\n",
    "            - Output only a JSON list of objects. Each object must have exactly two keys: \n",
    "              \"entity\" (the entity name) and \"category\" (the classified category).\n",
    "            - Do not output into code block, print as raw text\n",
    "\n",
    "            Transaction Text:\n",
    "            {transaction_text}\n",
    "\n",
    "            Output Format **(Do not output into code block, print as raw text):**\n",
    "            Eg: \n",
    "            [   {{\"Transaction ID\": \"TXN0234\"}}\n",
    "                {{\n",
    "                    \"entity\": \"Acme Corp\",\n",
    "                    \"category\": \"Corporation\"\n",
    "                }},\n",
    "                {{\n",
    "                    \"entity\": \"SovCo Capital Partners\",\n",
    "                    \"category\": \"Corporation\"\n",
    "                }}\n",
    "            ]\n",
    "        \"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                completion = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "                        }\n",
    "                    ]\n",
    "                )\n",
    "                extracted_text = completion.choices[0].message.content\n",
    "                extracted_entities = json.loads(extracted_text)\n",
    "                return extracted_entities\n",
    "\n",
    "            except Exception:\n",
    "                lines = extracted_text.splitlines()\n",
    "                if len(lines) > 2:\n",
    "                    trimmed = \"\\n\".join(lines[1:-1]).strip()\n",
    "                    try:\n",
    "                        extracted_entities = json.loads(trimmed)\n",
    "                        return extracted_entities\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "\n",
    "        return {\"error\": \"Failed to parse entity extraction response\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "268b092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanyMatcher:\n",
    "    def __init__(self, database):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.raw_database = database  \n",
    "        self.database = [self.preprocess_text(name) for name in database]  # Preprocessed database\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        if self.database:\n",
    "            self.vectors = self.vectorizer.fit_transform(self.database)\n",
    "        else:\n",
    "            self.vectors = None \n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z0-9\\s]', '', text)  \n",
    "        text = re.sub(r'\\s+', ' ', text).strip() \n",
    "        \n",
    "        stopwords = {\"corporation\", \"limited\", \"ltd\", \"solutions\", \"technologies\", \n",
    "                     \"consulting\", \"consultancy\", \"services\", \"systems\", \"group\", \n",
    "                     \"inc\", \"pvt\", \"plc\", \"co\"}\n",
    "        \n",
    "        words = text.split()\n",
    "        words = [self.lemmatizer.lemmatize(word) for word in words if word not in stopwords]  # Apply lemmatization\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def get_top_cosine_matches(self, query, top_n=5):\n",
    "        if not self.database:  \n",
    "            return []\n",
    "\n",
    "        query_cleaned = self.preprocess_text(query)\n",
    "        query_vector = self.vectorizer.transform([query_cleaned])\n",
    "        similarity_scores = cosine_similarity(query_vector, self.vectors).flatten()\n",
    "\n",
    "        top_n = min(top_n, len(similarity_scores)) \n",
    "        top_indices = np.argsort(-similarity_scores)[:top_n]\n",
    "        top_matches = [(self.raw_database[i], similarity_scores[i]) for i in top_indices]\n",
    "\n",
    "        return top_matches\n",
    "\n",
    "    def apply_fuzzy_matching(self, query, candidates):\n",
    "        if not candidates:\n",
    "            return None, None\n",
    "\n",
    "        query_cleaned = self.preprocess_text(query)\n",
    "        best_match, best_score = max(\n",
    "            ((name, fuzz.ratio(query_cleaned, self.preprocess_text(name))) for name, _ in candidates),\n",
    "            key=lambda x: x[1]\n",
    "        )\n",
    "\n",
    "        return (best_match, best_score)\n",
    "\n",
    "    def find_best_match(self, query, top_n=5):\n",
    "        top_matches = self.get_top_cosine_matches(query, top_n)\n",
    "        best_match, best_score = self.apply_fuzzy_matching(query, top_matches)\n",
    "        \n",
    "        return best_match, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66aebce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanyScreening:\n",
    "    def __init__(self, ofac_list_file):\n",
    "        self.SEC_BASE_URL = \"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "        self.WIKIDATA_URL = \"https://query.wikidata.org/sparql\"\n",
    "        self.NEWS_API_KEY = \"b77ab45e7bab4db7bad3108d90de4b25\"\n",
    "        self.OFAC_LIST_FILE = ofac_list_file\n",
    "        with open(self.OFAC_LIST_FILE, \"r\") as f:\n",
    "            self.ofac_companies = [line.strip().lower() for line in f]\n",
    "        self.matcher = CompanyMatcher(self.ofac_companies)\n",
    "        \n",
    "    def check_sec_edgar(self, company_name):\n",
    "        params = {\"action\": \"getcompany\", \"company\": company_name, \"output\": \"atom\"}\n",
    "        headers = {\"User-Agent\": \"XXX (xxx@yyy.com)\"}\n",
    "        response = requests.get(self.SEC_BASE_URL, params = params, headers = headers)\n",
    "        if response.status_code == 200 and \"No matching companies\" not in response.text:\n",
    "            soup = BeautifulSoup(response.text, \"xml\")\n",
    "            cik_tag = soup.find(\"cik\")\n",
    "            edgar_profile_url = None\n",
    "            recent_8k_filings = []\n",
    "            \n",
    "            if cik_tag:\n",
    "                cik = cik_tag.text.strip()\n",
    "\n",
    "                three_years_ago = datetime.now() - timedelta(days=3*365)\n",
    "                filings = soup.find_all(\"entry\")\n",
    "\n",
    "                for filing in filings:\n",
    "                    title = filing.find(\"title\").text\n",
    "                    date_str = filing.find(\"updated\").text[:10]\n",
    "                    filing_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "                    if \"8-K\" in title and filing_date >= three_years_ago:\n",
    "                        recent_8k_filings.append(title)\n",
    "\n",
    "                return {\n",
    "                    \"SEC Registered\": \"Yes\",\n",
    "                    \"CIK\": cik if cik_tag else \"Not available\",\n",
    "                    \"Recent 8-K Filings\": recent_8k_filings[:3] if recent_8k_filings else \"None\"\n",
    "                }\n",
    "        return {\n",
    "            \"SEC Registered\": \"No\"\n",
    "        }\n",
    "\n",
    "    def check_ofac_sanctions(self, company_name, threshold=85):\n",
    "        match, score = self.matcher.find_best_match(company_name)\n",
    "        return {\n",
    "            \"OFAC Sanctioned\": \"Yes\" if score >= threshold else \"No\",\n",
    "            \"Closest OFAC Database Match\": match if score >= threshold else \"None\",\n",
    "        }\n",
    "\n",
    "    def check_wikidata_scandals(self, company_name):\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT ?company ?companyLabel ?industry ?industryLabel ?scandal ?scandalLabel ?description WHERE {{\n",
    "          ?company rdfs:label \"{company_name}\"@en.\n",
    "          ?company wdt:P31 wd:Q4830453.  # Instance of (Business/Company)\n",
    "          OPTIONAL {{ ?company wdt:P452 ?industry. }}  # Industry type\n",
    "          OPTIONAL {{ ?company schema:description ?description. FILTER (LANG(?description) = \"en\") }}\n",
    "\n",
    "          # Looking for scandals\n",
    "          OPTIONAL {{ ?company wdt:P793 ?scandal. }}  # Significant events (may include fraud cases, controversies)\n",
    "          OPTIONAL {{ ?company wdt:P5053 ?scandal. }} # Cause of dissolution (bankruptcy, fraud)\n",
    "          OPTIONAL {{ ?company wdt:P2416 ?scandal. }} # Scandals (direct connection)\n",
    "\n",
    "          SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        response = requests.get(self.WIKIDATA_URL, params={\"query\": query, \"format\": \"json\"})\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            return {\"Error\": \"Failed to fetch data\"}\n",
    "\n",
    "        data = response.json().get(\"results\", {}).get(\"bindings\", [])\n",
    "\n",
    "        if not data:\n",
    "            return {\"Status\": \"Not Found in Wikidata\"}\n",
    "\n",
    "        result = data[0] \n",
    "        company_qid = result[\"company\"][\"value\"].split(\"/\")[-1]\n",
    "        \n",
    "        description = result.get(\"description\", {}).get(\"value\", \"No description available\")\n",
    "        if \"scandal\" in result:\n",
    "            scandal_name = result[\"scandalLabel\"][\"value\"]\n",
    "            scandal_qid = result[\"scandal\"][\"value\"].split(\"/\")[-1]\n",
    "            scandal_link = f\"https://www.wikidata.org/wiki/{scandal_qid}\"\n",
    "\n",
    "        return {\n",
    "            \"Wikidata_QID\": company_qid,\n",
    "            \"Description\": description,\n",
    "            \"Scandals\": (scandal_name, scandal_link) if \"scandal\" in result else \"No known scandals\",\n",
    "        }\n",
    "    def get_news(self, company_name):\n",
    "        url = f\"https://newsapi.org/v2/everything?q={company_name}&language=en&sortBy=publishedAt&apiKey={self.NEWS_API_KEY}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            return {\"Recent News\": \"Error fetching news\"}\n",
    "\n",
    "        news_data = response.json()\n",
    "        articles = news_data.get(\"articles\", [])\n",
    "\n",
    "        filtered_articles = [article[\"title\"] for article in articles if company_name.lower() in article[\"title\"].lower()]\n",
    "        return {\"Recent News\": filtered_articles[:3] if filtered_articles else \"No relevant news found\"}\n",
    "\n",
    "    def screen_company(self, company_name):\n",
    "        result = {\"Company\": company_name}\n",
    "        result.update(self.check_sec_edgar(company_name))\n",
    "        result.update(self.check_ofac_sanctions(company_name))\n",
    "        result.update(self.check_wikidata_scandals(company_name))\n",
    "        result.update(self.get_news(company_name))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f50794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransactionAnalyzer:\n",
    "    def __init__(self, ofac_list_file):\n",
    "        self.entity_extractor = EntityExtractor()\n",
    "        self.company_screening = CompanyScreening(ofac_list_file)\n",
    "\n",
    "    def analyze_transaction(self, transaction_text):\n",
    "        extracted_entities = self.entity_extractor.extract_entities(transaction_text)\n",
    "\n",
    "        transaction_id = None\n",
    "        for entity in extracted_entities:\n",
    "            if \"Transaction ID\" in entity:\n",
    "                transaction_id = entity[\"Transaction ID\"]\n",
    "                break\n",
    "\n",
    "        corporation_details = {\"Corporation Details\": []}\n",
    "        for entity in extracted_entities:\n",
    "            if entity.get(\"category\") == \"Corporation\":\n",
    "                company_details = self.company_screening.screen_company(entity[\"entity\"])\n",
    "                corporation_details[\"Corporation Details\"].append(company_details)\n",
    "\n",
    "        extracted_entities_list = [entity[\"entity\"] for entity in extracted_entities if \"entity\" in entity]\n",
    "        entity_types = [entity[\"category\"] for entity in extracted_entities if \"category\" in entity]\n",
    "\n",
    "        transaction_summary = {\n",
    "            \"Transaction ID\": transaction_id,\n",
    "            \"Extracted Entities\": extracted_entities_list,\n",
    "            \"Entity Type\": entity_types\n",
    "        }\n",
    "\n",
    "        return json.dumps({\"corporation_details\": corporation_details, \"transaction_summary\": transaction_summary}, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.api_key = os.getenv(\"TogetherAI_API_Key\") \n",
    "        self.api_url = \"https://api.together.xyz/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "    def analyze_risk(self, input_texts, output_texts, test_input):\n",
    "\n",
    "        examples = random.sample(list(zip(input_texts, output_texts)), 50)\n",
    "        icl_prompt = \"\".join([f\"Input: {inp}\\nOutput: {out}\\n\\n\" for inp, out in examples])\n",
    "        icl_prompt += f\"\"\"Input: {test_input}\\nOutput (JSON Format): **Provide Reason in atleast 5 lines with atleast one sentence on transaction summary identifying tax havens** **If no data is found in corporate details, mention in the reason that provided risk score is only based on transaction data since no external data sources are found**\n",
    "        **Do not change Supporting Evidence** **Provide low confidence score between if external data sources do not provide any information**\n",
    "        {{\n",
    "            \"Transaction ID\": \"...\",\n",
    "            \"Extracted Entities\": [...],\n",
    "            \"Entity Type\": [...],\n",
    "            \"Risk Score\": ...,\n",
    "            \"Confidence Score\": ...,\n",
    "            \"Reason\": \"...\"\n",
    "            \"Supporting Evidence\": [\"SEC EDGAR\", \"OFAC Sanctions List\", \"News Articles\", \"Wikidata\"]\n",
    "        }}\n",
    "        **If no data is found in corporate details, mention in the reason that provided risk score is only based on transaction data since no external data sources are found**\n",
    "   \n",
    "        \"\"\"\n",
    "\n",
    "        data = {\n",
    "            \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": icl_prompt}],\n",
    "        }\n",
    "        \n",
    "        response = requests.post(self.api_url, headers=self.headers, json=data)\n",
    "        \n",
    "        try:\n",
    "            return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except KeyError:\n",
    "            return f\"Error: {response.json()}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b7bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corporate_details.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "    details= [t.strip() for t in content.split('---') if t.strip()]\n",
    "    \n",
    "with open(\"synthetic_unstructured_data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "    transactions = [t.strip() for t in content.split('---') if t.strip()]\n",
    "    \n",
    "with open(\"risk_summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "    risk= [t.strip() for t in content.split('---') if t.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "239e8758",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_inputs = [f\"{a1} {a2}\" for a1, a2 in zip(transactions[:56], details[:56])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "155c8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unstructured_data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "    transactions = [t.strip() for t in content.split('---') if t.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "612d0b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"structured_data.csv\", newline = \"\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if \"TransactionText\" in row:\n",
    "            txn_text = row[\"TransactionText\"]\n",
    "        else:\n",
    "            txn_text = \"\\n\".join(f\"{key}: {value}\" for key, value in row.items())\n",
    "            transactions.append(txn_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "923d0ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "    \"Transaction ID\": \"TXN-2023-5A95\",\n",
      "    \"Extracted Entities\": [\"Global Horizona Consulting LLC\", \"Swiss Bank\", \"Bright Future Nonprofit Inc\", \"Cayman National Bank\", \"Mr. Ali Al-Mansoori\", \"Quantum Holding Ltd\"],\n",
      "    \"Entity Type\": [\"Corporation\", \"Bank\", \"Non-Profit Organization\", \"Bank\", \"Person\", \"Shell Company\"],\n",
      "    \"Risk Score\": 0.6,\n",
      "    \"Confidence Score\": 0.5,\n",
      "    \"Reason\": \"The transaction involves a wire transfer from a Swiss bank to a Cayman Islands-based bank, both of which are known tax havens. The sender, Global Horizona Consulting LLC, has no external data sources found, making the risk score based only on transaction data. The urgency of the transfer, approval by a director, and the use of a VPN further raise concerns.\",\n",
      "    \"Supporting Evidence\": [\"Transaction Data\", \"SEC EDGAR\", \"OFAC Sanctions List\", \"News Articles\", \"Wikidata\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "analyzer = TransactionAnalyzer(\"ofac_list.txt\")\n",
    "result_json = analyzer.analyze_transaction(transactions[0])\n",
    "corporation_details = json.loads(result_json)[\"corporation_details\"]\n",
    "risk_model_input = f\"{transactions[0]}\\n{result_json}\"\n",
    "analyzer = RiskAnalyzer()\n",
    "result = analyzer.analyze_risk(combined_inputs[:56], risk, risk_model_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fac7be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "    \"Transaction ID\": \"TKN-2023-7020\",\n",
      "    \"Extracted Entities\": [\"Quantum Holdings Ltd\", \"Maria Gonzalez\", \"Golden Sands Trading FZE\", \"Deutsche Bank\", \"Emirates NBD\", \"Mr. Viktor Petzov\"],\n",
      "    \"Entity Type\": [\"Corporation\", \"Person\", \"Corporation\", \"Bank\", \"Bank\", \"Politically Exposed Person\"],\n",
      "    \"Risk Score\": 0.8,\n",
      "    \"Confidence Score\": 0.5,\n",
      "    \"Reason\": \"The transaction involves Quantum Holdings Ltd, a corporation based in the British Virgin Islands, a known tax haven. The funds are routed through Deutsche Bank Frankfurt and Emirates NBD Dubai. The approver, Mr. Viktor Petzov, is linked to an OFAC SDN List entry in 2022. Due to the lack of information from external data sources, the risk score is only based on transaction data.\",\n",
      "    \"Supporting Evidence\": [\"SEC EDGAR\", \"OFAC Sanctions List\", \"News Articles\", \"Wikidata\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "analyzer = TransactionAnalyzer(\"ofac_list.txt\")\n",
    "result_json = analyzer.analyze_transaction(transactions[1])\n",
    "risk_model_input = f\"{transactions[1]}\\n{result_json}\"\n",
    "result = RiskAnalyzer().analyze_risk(combined_inputs[:56], risk, risk_model_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a6c0a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "    \"Transaction ID\": \"TXN001\",\n",
      "    \"Extracted Entities\": [\"Acme Corp\", \"SovCo Capital Partners\"],\n",
      "    \"Entity Type\": [\"Corporation\", \"Corporation\"],\n",
      "    \"Risk Score\": 0.4,\n",
      "    \"Confidence Score\": 0.5,\n",
      "    \"Reason\": \"Based on the provided transaction data, the risk score is 0.4. No external data sources are found for Acme Corp and SovCo Capital Partners. Therefore, the reason is only based on the transaction data.\",\n",
      "    \"Supporting Evidence\": [\"SEC EDGAR\", \"OFAC Sanctions List\", \"News Articles\", \"Wikidata\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result_json = analyzer.analyze_transaction(transactions[2])\n",
    "risk_model_input = f\"{transactions[2]}\\n{result_json}\"\n",
    "analyzer = RiskAnalyzer()\n",
    "result = analyzer.analyze_risk(combined_inputs[:56], risk, risk_model_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8070616f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "    \"Transaction ID\": \"TXN002\",\n",
      "    \"Extracted Entities\": [\"Global Health Foundation\", \"Save the Children\"],\n",
      "    \"Entity Type\": [\"Non-Profit Organization\", \"Non-Profit Organization\"],\n",
      "    \"Risk Score\": 0.2,\n",
      "    \"Confidence Score\": 0.5,\n",
      "    \"Reason\": \"Transaction summary does not indicate any high-risk activities. However, the risk score is provided based solely on transaction data since no external data sources are found for the entities involved. No tax havens are identified in this transaction.\",\n",
      "    \"Supporting Evidence\": [\"Transaction Summary\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "analyzer = TransactionAnalyzer(\"ofac_list.txt\")\n",
    "result_json = analyzer.analyze_transaction(transactions[3])\n",
    "risk_model_input = f\"{transactions[3]}\\n{result_json}\"\n",
    "analyzer = RiskAnalyzer()\n",
    "result = analyzer.analyze_risk(combined_inputs[:56], risk, risk_model_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "224ae28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "    \"Transaction ID\": \"TXN003\",\n",
      "    \"Extracted Entities\": [\"XYZ Ltd\", \"ABC GmbH\"],\n",
      "    \"Entity Type\": [\"Corporation\", \"Corporation\"],\n",
      "    \"Risk Score\": 0.3,\n",
      "    \"Confidence Score\": 0.5,\n",
      "    \"Reason\": \"The transaction is a standard purchase of office supplies between two entities with no red flags in the provided data. However, the risk score is provided based solely on transaction data since no external data sources such as SEC EDGAR, OFAC Sanctions List, News Articles, or Wikidata have any information on these entities.\",\n",
      "    \"Supporting Evidence\": [\"SEC EDGAR\", \"OFAC Sanctions List\", \"News Articles\", \"Wikidata\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "analyzer = TransactionAnalyzer(\"ofac_list.txt\")\n",
    "result_json = analyzer.analyze_transaction(transactions[4])\n",
    "risk_model_input = f\"{transactions[4]}\\n{result_json}\"\n",
    "analyzer = RiskAnalyzer()\n",
    "result = analyzer.analyze_risk(combined_inputs[:56], risk, risk_model_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "030f43de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "    \"Transaction ID\": \"TXN004\",\n",
      "    \"Extracted Entities\": [\"Green Earth Org\", \"CCMI\"],\n",
      "    \"Entity Type\": [\"Non-Profit Organization\", \"Corporation\"],\n",
      "    \"Risk Score\": 0.5,\n",
      "    \"Confidence Score\": 0.6,\n",
      "    \"Reason\": \"The transaction involves funding for an environmental project with a payer being a non-profit organization and the receiver being a corporation based in the Cayman Islands, a known tax haven. The provided risk score is based on transaction data since no external data sources are found for CCMI.\",\n",
      "    \"Supporting Evidence\": [\"Transaction Data\", \"SEC EDGAR\", \"OFAC Sanctions List\", \"News Articles\", \"Wikidata\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "analyzer = TransactionAnalyzer(\"ofac_list.txt\")\n",
    "result_json = analyzer.analyze_transaction(transactions[5])\n",
    "risk_model_input = f\"{transactions[5]}\\n{result_json}\"\n",
    "analyzer = RiskAnalyzer()\n",
    "result = analyzer.analyze_risk(combined_inputs[:56], risk, risk_model_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e1e6e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "    \"Transaction ID\": \"TXN005\",\n",
      "    \"Extracted Entities\": [\"Oceanic Holdings LLC\", \"Alas Chiricanas\"],\n",
      "    \"Entity Type\": [\"Corporation\", \"Corporation\"],\n",
      "    \"Risk Score\": 0.8,\n",
      "    \"Confidence Score\": 0.5,\n",
      "    \"Reason\": \"Oceanic Holdings LLC appears in the OFAC sanctions list, indicating potential financial misconduct. The transaction involves offshore investment in Panama, a known tax haven. However, no additional information is found in external data sources for either corporation, making it difficult to assess associated risks accurately.\",\n",
      "    \"Supporting Evidence\": [\"SEC EDGAR\", \"OFAC Sanctions List\", \"News Articles\", \"Wikidata\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "analyzer = TransactionAnalyzer(\"ofac_list.txt\")\n",
    "result_json = analyzer.analyze_transaction(transactions[6])\n",
    "risk_model_input = f\"{transactions[6]}\\n{result_json}\"\n",
    "analyzer = RiskAnalyzer()\n",
    "result = analyzer.analyze_risk(combined_inputs[:56], risk, risk_model_input)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
