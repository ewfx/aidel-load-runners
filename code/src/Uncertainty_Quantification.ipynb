{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63444ecb",
   "metadata": {},
   "source": [
    "<H2><center>Uncertainty Quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c3bc24",
   "metadata": {},
   "source": [
    "\n",
    "<H2>Description</H2>This notebook presents a method to quantify confidence score based on uncertainty in the predicted risk score. Since external data sources (SEC EDGAR, OFAC, WikiData) are deterministic, uncertainty propagates only from the LLM-predicted risk score.\n",
    "\n",
    "\n",
    "### **Numerical Quantification**\n",
    "- The LLM prediction is repeated **5 times**, and the **standard deviation** of the scores indicates uncertainty.\n",
    "\n",
    "### **Scaling Confidence Score**\n",
    "- Since risk values lie in **[0,1]**, the standard deviation (**σ**) falls within **0 < σ < 0.5**.\n",
    "- The confidence score is computed as:\n",
    "  **Confidence Score = 1 - 2σ**\n",
    "  \n",
    "- If all predictions are identical (**σ = 0**), confidence is **1**.\n",
    "- If predictions vary widely (**σ ≈ 0.5**), confidence is **0**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5fe324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "from thefuzz import fuzz, process\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "820d96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityExtractor:\n",
    "    def __init__(self, base_url=\"https://openrouter.ai/api/v1\"):\n",
    "        self.client = OpenAI(\n",
    "            base_url=base_url,\n",
    "            api_key=\"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\")\n",
    "        self.model = \"mistralai/mistral-small-3.1-24b-instruct:free\"\n",
    "        self.max_retries = 5\n",
    "        \n",
    "    def extract_entities(self, transaction_text: str) -> list:\n",
    "        prompt_text = f\"\"\"\n",
    "            Extract transaction id and all named entities from the following transaction text and classify each entity into one of these categories:\n",
    "            Person, Politically Exposed Person, Corporation, Bank, Government Agency, Non-Profit Organization, or Shell Company.\n",
    "            - If the entity's category is ambiguous, classify it as \"Corporation\".\n",
    "            - Do NOT include IBANs, VPNs, IPs, addresses, account numbers, tax IDs, location, cities, or countries.\n",
    "            - Banks should be extracted separately (e.g., \"Swiss Bank\", \"Cayman National Bank\") without IBAN/account numbers.\n",
    "            - Include people with titles (Mr., Mrs., Dr., etc.) as Person.\n",
    "            - Output only a JSON list of objects. Each object must have exactly two keys: \n",
    "              \"entity\" (the entity name) and \"category\" (the classified category).\n",
    "            - Do not output into code block, print as raw text\n",
    "\n",
    "            Transaction Text:\n",
    "            {transaction_text}\n",
    "\n",
    "            Output Format **(Do not output into code block, print as raw text):**\n",
    "            Eg: \n",
    "            [   {{\"Transaction ID\": \"TXN0234\"}}\n",
    "                {{\n",
    "                    \"entity\": \"Acme Corp\",\n",
    "                    \"category\": \"Corporation\"\n",
    "                }},\n",
    "                {{\n",
    "                    \"entity\": \"SovCo Capital Partners\",\n",
    "                    \"category\": \"Corporation\"\n",
    "                }}\n",
    "            ]\n",
    "        \"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                completion = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "                        }\n",
    "                    ]\n",
    "                )\n",
    "                extracted_text = completion.choices[0].message.content\n",
    "                extracted_entities = json.loads(extracted_text)\n",
    "                return extracted_entities\n",
    "\n",
    "            except Exception:\n",
    "                lines = extracted_text.splitlines()\n",
    "                if len(lines) > 2:\n",
    "                    trimmed = \"\\n\".join(lines[1:-1]).strip()\n",
    "                    try:\n",
    "                        extracted_entities = json.loads(trimmed)\n",
    "                        return extracted_entities\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "\n",
    "        return {\"error\": \"Failed to parse entity extraction response\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "268b092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanyMatcher:\n",
    "    def __init__(self, database):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.raw_database = database  \n",
    "        self.database = [self.preprocess_text(name) for name in database]  # Preprocessed database\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        if self.database:\n",
    "            self.vectors = self.vectorizer.fit_transform(self.database)\n",
    "        else:\n",
    "            self.vectors = None \n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z0-9\\s]', '', text)  \n",
    "        text = re.sub(r'\\s+', ' ', text).strip() \n",
    "        \n",
    "        stopwords = {\"corporation\", \"limited\", \"ltd\", \"solutions\", \"technologies\", \n",
    "                     \"consulting\", \"consultancy\", \"services\", \"systems\", \"group\", \n",
    "                     \"inc\", \"pvt\", \"plc\", \"co\"}\n",
    "        \n",
    "        words = text.split()\n",
    "        words = [self.lemmatizer.lemmatize(word) for word in words if word not in stopwords]  # Apply lemmatization\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def get_top_cosine_matches(self, query, top_n=5):\n",
    "        if not self.database:  \n",
    "            return []\n",
    "\n",
    "        query_cleaned = self.preprocess_text(query)\n",
    "        query_vector = self.vectorizer.transform([query_cleaned])\n",
    "        similarity_scores = cosine_similarity(query_vector, self.vectors).flatten()\n",
    "\n",
    "        top_n = min(top_n, len(similarity_scores)) \n",
    "        top_indices = np.argsort(-similarity_scores)[:top_n]\n",
    "        top_matches = [(self.raw_database[i], similarity_scores[i]) for i in top_indices]\n",
    "\n",
    "        return top_matches\n",
    "\n",
    "    def apply_fuzzy_matching(self, query, candidates):\n",
    "        if not candidates:\n",
    "            return None, None\n",
    "\n",
    "        query_cleaned = self.preprocess_text(query)\n",
    "        best_match, best_score = max(\n",
    "            ((name, fuzz.ratio(query_cleaned, self.preprocess_text(name))) for name, _ in candidates),\n",
    "            key=lambda x: x[1]\n",
    "        )\n",
    "\n",
    "        return (best_match, best_score)\n",
    "\n",
    "    def find_best_match(self, query, top_n=5):\n",
    "        top_matches = self.get_top_cosine_matches(query, top_n)\n",
    "        best_match, best_score = self.apply_fuzzy_matching(query, top_matches)\n",
    "        \n",
    "        return best_match, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66aebce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanyScreening:\n",
    "    def __init__(self, ofac_list_file):\n",
    "        self.SEC_BASE_URL = \"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "        self.WIKIDATA_URL = \"https://query.wikidata.org/sparql\"\n",
    "        self.NEWS_API_KEY = \"b77ab45e7bab4db7bad3108d90de4b25\"\n",
    "        self.OFAC_LIST_FILE = ofac_list_file\n",
    "        with open(self.OFAC_LIST_FILE, \"r\") as f:\n",
    "            self.ofac_companies = [line.strip().lower() for line in f]\n",
    "        self.matcher = CompanyMatcher(self.ofac_companies)\n",
    "        \n",
    "    def check_sec_edgar(self, company_name):\n",
    "        params = {\"action\": \"getcompany\", \"company\": company_name, \"output\": \"atom\"}\n",
    "        headers = {\"User-Agent\": \"XXX (xxx@yyy.com)\"}\n",
    "        response = requests.get(self.SEC_BASE_URL, params = params, headers = headers)\n",
    "        if response.status_code == 200 and \"No matching companies\" not in response.text:\n",
    "            soup = BeautifulSoup(response.text, \"xml\")\n",
    "            cik_tag = soup.find(\"cik\")\n",
    "            edgar_profile_url = None\n",
    "            recent_8k_filings = []\n",
    "            \n",
    "            if cik_tag:\n",
    "                cik = cik_tag.text.strip()\n",
    "\n",
    "                three_years_ago = datetime.now() - timedelta(days=3*365)\n",
    "                filings = soup.find_all(\"entry\")\n",
    "\n",
    "                for filing in filings:\n",
    "                    title = filing.find(\"title\").text\n",
    "                    date_str = filing.find(\"updated\").text[:10]\n",
    "                    filing_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "                    if \"8-K\" in title and filing_date >= three_years_ago:\n",
    "                        recent_8k_filings.append(title)\n",
    "\n",
    "                return {\n",
    "                    \"SEC Registered\": \"Yes\",\n",
    "                    \"CIK\": cik if cik_tag else \"Not available\",\n",
    "                    \"Recent 8-K Filings\": recent_8k_filings[:3] if recent_8k_filings else \"None\"\n",
    "                }\n",
    "        return {\n",
    "            \"SEC Registered\": \"No\"\n",
    "        }\n",
    "\n",
    "    def check_ofac_sanctions(self, company_name, threshold=85):\n",
    "        match, score = self.matcher.find_best_match(company_name)\n",
    "        return {\n",
    "            \"OFAC Sanctioned\": \"Yes\" if score >= threshold else \"No\",\n",
    "            \"Closest OFAC Database Match\": match if score >= threshold else \"None\",\n",
    "        }\n",
    "\n",
    "    def check_wikidata_scandals(self, company_name):\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT ?company ?companyLabel ?industry ?industryLabel ?scandal ?scandalLabel ?description WHERE {{\n",
    "          ?company rdfs:label \"{company_name}\"@en.\n",
    "          ?company wdt:P31 wd:Q4830453.  # Instance of (Business/Company)\n",
    "          OPTIONAL {{ ?company wdt:P452 ?industry. }}  # Industry type\n",
    "          OPTIONAL {{ ?company schema:description ?description. FILTER (LANG(?description) = \"en\") }}\n",
    "\n",
    "          # Looking for scandals\n",
    "          OPTIONAL {{ ?company wdt:P793 ?scandal. }}  # Significant events (may include fraud cases, controversies)\n",
    "          OPTIONAL {{ ?company wdt:P5053 ?scandal. }} # Cause of dissolution (bankruptcy, fraud)\n",
    "          OPTIONAL {{ ?company wdt:P2416 ?scandal. }} # Scandals (direct connection)\n",
    "\n",
    "          SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        response = requests.get(self.WIKIDATA_URL, params={\"query\": query, \"format\": \"json\"})\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            return {\"Error\": \"Failed to fetch data\"}\n",
    "\n",
    "        data = response.json().get(\"results\", {}).get(\"bindings\", [])\n",
    "\n",
    "        if not data:\n",
    "            return {\"Status\": \"Not Found in Wikidata\"}\n",
    "\n",
    "        result = data[0] \n",
    "        company_qid = result[\"company\"][\"value\"].split(\"/\")[-1]\n",
    "        \n",
    "        description = result.get(\"description\", {}).get(\"value\", \"No description available\")\n",
    "        if \"scandal\" in result:\n",
    "            scandal_name = result[\"scandalLabel\"][\"value\"]\n",
    "            scandal_qid = result[\"scandal\"][\"value\"].split(\"/\")[-1]\n",
    "            scandal_link = f\"https://www.wikidata.org/wiki/{scandal_qid}\"\n",
    "\n",
    "        return {\n",
    "            \"Wikidata_QID\": company_qid,\n",
    "            \"Description\": description,\n",
    "            \"Scandals\": (scandal_name, scandal_link) if \"scandal\" in result else \"No known scandals\",\n",
    "        }\n",
    "    def get_news(self, company_name):\n",
    "        url = f\"https://newsapi.org/v2/everything?q={company_name}&language=en&sortBy=publishedAt&apiKey={self.NEWS_API_KEY}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            return {\"Recent News\": \"Error fetching news\"}\n",
    "\n",
    "        news_data = response.json()\n",
    "        articles = news_data.get(\"articles\", [])\n",
    "\n",
    "        filtered_articles = [article[\"title\"] for article in articles if company_name.lower() in article[\"title\"].lower()]\n",
    "        return {\"Recent News\": filtered_articles[:3] if filtered_articles else \"No relevant news found\"}\n",
    "\n",
    "    def screen_company(self, company_name):\n",
    "        result = {\"Company\": company_name}\n",
    "        result.update(self.check_sec_edgar(company_name))\n",
    "        result.update(self.check_ofac_sanctions(company_name))\n",
    "        result.update(self.check_wikidata_scandals(company_name))\n",
    "        result.update(self.get_news(company_name))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f50794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransactionAnalyzer:\n",
    "    def __init__(self, ofac_list_file):\n",
    "        self.entity_extractor = EntityExtractor()\n",
    "        self.company_screening = CompanyScreening(ofac_list_file)\n",
    "\n",
    "    def analyze_transaction(self, transaction_text):\n",
    "        extracted_entities = self.entity_extractor.extract_entities(transaction_text)\n",
    "\n",
    "        transaction_id = None\n",
    "        for entity in extracted_entities:\n",
    "            if \"Transaction ID\" in entity:\n",
    "                transaction_id = entity[\"Transaction ID\"]\n",
    "                break\n",
    "\n",
    "        corporation_details = {\"Corporation Details\": []}\n",
    "        for entity in extracted_entities:\n",
    "            if entity.get(\"category\") == \"Corporation\":\n",
    "                company_details = self.company_screening.screen_company(entity[\"entity\"])\n",
    "                corporation_details[\"Corporation Details\"].append(company_details)\n",
    "\n",
    "        extracted_entities_list = [entity[\"entity\"] for entity in extracted_entities if \"entity\" in entity]\n",
    "        entity_types = [entity[\"category\"] for entity in extracted_entities if \"category\" in entity]\n",
    "\n",
    "        transaction_summary = {\n",
    "            \"Transaction ID\": transaction_id,\n",
    "            \"Extracted Entities\": extracted_entities_list,\n",
    "            \"Entity Type\": entity_types\n",
    "        }\n",
    "\n",
    "        return json.dumps({\"corporation_details\": corporation_details, \"transaction_summary\": transaction_summary}, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb4e896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.api_key = \"7f4f86c39168c8929cf40312d13d926e94fc4fd7f22293a75085761c2f55a263\" \n",
    "        self.api_url = \"https://api.together.xyz/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "    def analyze_risk(self, test_input):\n",
    "\n",
    "        icl_prompt = f\"\"\"Input: {test_input}\\nOutput (Float): **Output only risk score between 0 and 1 without additional details***\n",
    "        Output format: 0.xx **Dont print reason. Score each transaction independently**\n",
    "        \"\"\"\n",
    "            \n",
    "        data = {\n",
    "            \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": icl_prompt}],\n",
    "        }\n",
    "        \n",
    "        response = requests.post(self.api_url, headers=self.headers, json=data)\n",
    "        \n",
    "        try:\n",
    "            return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except KeyError:\n",
    "            return f\"Error: {response.json()}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b7bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_transaction = \"\"\" Transaction ID: TXN-2023-5A95\n",
    "\n",
    "Date: 2023-08-15 14:22:00\n",
    "\n",
    "Sender:\n",
    "Name: Global Horizona Consulting LLC\n",
    "Account: IBAN CH56 0483 5012 3456 7800 9 (Swiss bank)\n",
    "Address: Rue du Marché 17, Geneva, Switzerland\n",
    "Notes: Consulting fees for project Aurora\n",
    "\n",
    "Receiver:\n",
    "Name: Bright Future Nonprofit Inc\n",
    "Account: 987654321 (Cayman National Bank, KYJ)\n",
    "Address: P.O. Box 1234, George Town, Cayman Islands\n",
    "Tax ID: RY-45678\n",
    "\n",
    "Amount: $49,850.00 (USD)\n",
    "Currency Exchange: N/A\n",
    "Transaction Type: Wire Transfer\n",
    "Reference: Charitable Donation Ref #DR-2023-0815\n",
    "\n",
    "Additional Notes:\n",
    "Urgent transfer approved by Mr. Ali Al-Mansoori (Director).\n",
    "Linked invoice missing. Processed via intermediary Quantum Holding.\n",
    "\n",
    "Sender IP: 192.168.09.123 (VPN detected: NordVPN, exit node in Panama)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "923d0ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = []\n",
    "for i in range(5):\n",
    "    analyzer = TransactionAnalyzer(\"ofac_list.txt\")\n",
    "    result_json = analyzer.analyze_transaction(sample_transaction)\n",
    "    corporation_details = json.loads(result_json)[\"corporation_details\"]\n",
    "    risk_model_input = f\"{sample_transaction}\\n{result_json}\"\n",
    "    analyzer = RiskAnalyzer()\n",
    "    result = analyzer.analyze_risk(risk_model_input)\n",
    "    matches = re.findall(r\"\\d+\\.\\d+\", result)\n",
    "    risk_score = [float(m) for m in matches]\n",
    "    risk.append(risk_score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fac7be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Predicted Risk Scores: [0.6, 0.55, 0.65, 0.7, 0.72]\n"
     ]
    }
   ],
   "source": [
    "print(f\"LLM Predicted Risk Scores: {risk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a6c0a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation: 0.06\n",
      "Confidence Score: 0.87\n"
     ]
    }
   ],
   "source": [
    "std_dev = np.std(risk)\n",
    "confidence_score = 1 - 2 * std_dev\n",
    "print(\"Standard Deviation:\", round(std_dev,2))\n",
    "print(\"Confidence Score:\", round(confidence_score,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
